#!/usr/bin/env python
import sys
from collections import deque

import rospy

from threading import Lock

import numpy as np
import torch
import faster_whisper

import time

from audio_common_msgs.msg import AudioData, AudioInfo
from std_msgs.msg import String


class WhisperNode:
    def __init__(self, model_or_path, language="en", compute_type="default"):

        self.pub = rospy.Publisher("~text", String, queue_size=1)
        self.max_length = 8.0

        rospy.loginfo(logger_name="Whisper", msg=f"waiting for audio_info...")
        audio_info: AudioInfo = rospy.wait_for_message(
            "audio_info", AudioInfo, timeout=5
        )

        if not audio_info.sample_rate == 16000:
            msg = f"Sample rate needs to be 16000"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        if not audio_info.channels == 1:
            msg = f"Cant use non mono audio"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        if audio_info.sample_format.lower() == "s16le":
            self.depth_type = np.int16
            self.normalization = float(np.iinfo(self.depth_type).max)
        elif audio_info.sample_format.lower() == "f32le":
            self.depth_type = np.float32
            self.normalization = None
        else:
            msg = f"unhandled audio format"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        rospy.loginfo("initializing whisper...")
        self.device_ = "cpu"
        if torch.cuda.is_available():
            rospy.loginfo("CUDA is available. Using GPU.")
            self.device_ = "cuda"
        self.whisper_model_ = faster_whisper.WhisperModel(
            model_or_path, device=self.device_, compute_type=compute_type
        )
        rospy.logdebug("warmup...")
        sampl = np.random.uniform(low=0.0, high=1, size=(1000,))
        self.whisper_model_.transcribe(sampl)
        rospy.loginfo("Whisper initialized")

    def transcribe_audio(self, data: AudioData):
        rospy.loginfo(f"starting transcription..")
        start = time.time()
        audio_data = np.frombuffer(data.data, dtype=self.depth_type)
        if self.normalization is None:
            pass
        else:
            audio_data = np.divide(audio_data, self.normalization)
        # audio = whisper.pad_or_trim(audio)
        # mel = whisper.log_mel_spectrogram(audio, self.n_mels, device=self.device_).to(
        #    self.device_
        # )
        # result = whisper.decode(self.whisper_model_, mel, self.whisper_options_)
        segments, info = self.whisper_model_.transcribe(audio_data, beam_size=5)
        if info.all_language_probs is not None:
            for language in info.all_language_probs:
                print(
                    f"Detected language '{language[0]}' with probability {language[1]:1.4f}"
                )
                if language[1] < 0.01:
                    break
        # print("SEGMENTS:")
        end = time.time()
        for segment in segments:
            print(f"  {segment}")
            if segment.no_speech_prob < 0.7:
                rospy.loginfo(f"speech result: '{segment.text}' , took: {end - start}")
                self.pub.publish(segment.text)
            else:
                rospy.logwarn(
                    f"no speech {segment.no_speech_prob}, TEXT: {segment.text}"
                )


if __name__ == "__main__":

    # Start ROS node
    rospy.init_node("ros_whisper_transcribe")
    model_or_path = rospy.get_param("~model_or_path", "base.en")
    language = rospy.get_param("~language", "en")
    quant = rospy.get_param("~quant", "default")
    rospy.loginfo(f"using model: {model_or_path}")
    node = WhisperNode(model_or_path, language, quant)

    rospy.Subscriber("~input", AudioData, node.transcribe_audio)

    rospy.spin()
