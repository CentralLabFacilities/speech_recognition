#!/usr/bin/env python3
import sys
from collections import deque

import rospy

from threading import Lock

import numpy as np
import torch
import whisper

import time

from audio_common_msgs.msg import AudioData, AudioInfo
from std_msgs.msg import String


class WhisperNode:
    def __init__(self, model_or_path, language, n_mels):
        self.whisper_model_ = whisper.load_model(model_or_path)

        if language != "multi":
            self.whisper_options_ = whisper.DecodingOptions(language=language)
        else:
            self.whisper_options_ = whisper.DecodingOptions()

        self.pub = rospy.Publisher("~text", String, queue_size=1)
        self.n_mels = n_mels

        self.max_length = 8.0

        audio_info: AudioInfo = rospy.wait_for_message(
            "audio_info", AudioInfo, timeout=5
        )

        if not audio_info.sample_rate == 16000:
            msg = f"Sample rate needs to be 16000"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        if not audio_info.channels == 1:
            msg = f"Cant use non mono audio"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        if audio_info.sample_format.lower() == "s16le":
            self.depth_type = np.int16
            self.normalization = float(np.iinfo(self.depth_type).max)
        elif audio_info.sample_format.lower() == "f32le":
            self.depth_type = np.float32
            self.normalization = None
        else:
            msg = f"unhandled audio format"
            rospy.logerr(logger_name="Whisper", msg=msg)
            raise msg

        rospy.loginfo("initializing whisper...")
        self.device_ = "cpu"
        if torch.cuda.is_available():
            rospy.loginfo("CUDA is available. Using GPU.")
            self.device_ = "cuda"
        self.whisper_model_ = self.whisper_model_.to(self.device_)
        rospy.loginfo("Whisper initialized")

    def transcribe_audio(self, data: AudioData):
        rospy.loginfo(f"starting transcription..")
        start = time.time()
        audio_data = np.frombuffer(data.data, dtype=self.depth_type)
        if self.normalization is None:
            audio = torch.from_numpy(audio_data)
        else:
            audio = np.divide(audio_data, self.normalization)
            audio = torch.from_numpy(audio).float()
        audio = whisper.pad_or_trim(audio)
        mel = whisper.log_mel_spectrogram(audio, self.n_mels, device=self.device_).to(
            self.device_
        )
        result = whisper.decode(self.whisper_model_, mel, self.whisper_options_)
        end = time.time()
        if result.no_speech_prob < 0.7:
            rospy.loginfo(f"speech result: '{result.text}' , took: {end - start}")
            self.pub.publish(result.text)
        else:
            rospy.logwarn(f"no speech {result.no_speech_prob}, TEXT: {result.text}")


if __name__ == "__main__":

    # Start ROS node
    rospy.init_node("ros_whisper_transcribe")
    model_or_path = rospy.get_param("~model_or_path", "base.en")
    language = rospy.get_param("~language", "en")
    n_mels = rospy.get_param("~n_mels", 80)
    rospy.loginfo(f"using model: {model_or_path}")
    node = WhisperNode(model_or_path, language, n_mels)

    rospy.Subscriber("~input", AudioData, node.transcribe_audio)

    rospy.spin()
